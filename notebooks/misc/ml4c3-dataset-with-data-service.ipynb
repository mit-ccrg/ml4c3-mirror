{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "from ml4c3.tensormap.TensorMap import update_tmaps, PatientData\n",
    "from ml4c3.datasets import infer_mrn_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmaps = {}\n",
    "update_tmaps('ecg_2500_std_no_pacemaker_180_days_pre_echo', tmaps)\n",
    "update_tmaps('as_significant_180_days_post_ecg', tmaps)\n",
    "input_tmaps = [tmaps['ecg_2500_std_no_pacemaker_180_days_pre_echo']]\n",
    "output_tmaps = [tmaps['as_significant_180_days_post_ecg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"train\"\n",
    "hd5_sources = ['/storage/shared/ecg/mgh']\n",
    "csv_sources = [('/home/sn69/dropbox/ecgnet-as/data/echo.csv', 'echo')]\n",
    "patient_ids = set(pd.read_csv('/home/sn69/dropbox/ecgnet-as/data/test.csv')['patientid'])\n",
    "batch_size = 32\n",
    "num_workers = 20\n",
    "augment = False\n",
    "validate = True\n",
    "normalize = True\n",
    "\n",
    "tmaps = input_tmaps + output_tmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = []\n",
    "for csv_source, csv_name in csv_sources:\n",
    "    df = pd.read_csv(csv_source, low_memory=False)\n",
    "    mrn_col = infer_mrn_column(df, csv_source)\n",
    "    df[mrn_col] = df[mrn_col].dropna().astype(int)\n",
    "    csv_data.append((csv_name, df, mrn_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patient_tensors(patient_id):\n",
    "    open_hd5s = []\n",
    "    bad_idxs = []\n",
    "    tensors = [[] for tm in tmaps]\n",
    "    try:\n",
    "        data = PatientData(patient_id=patient_id)\n",
    "        # Add top level groups in hd5s to patient dictionary\n",
    "        for hd5_source in hd5_sources:\n",
    "            hd5_path = os.path.join(hd5_source, f\"{patient_id}.hd5\")\n",
    "            if not os.path.isfile(hd5_path):\n",
    "                continue\n",
    "            hd5 = h5py.File(hd5_path, \"r\")\n",
    "            for key in hd5:\n",
    "                data[key] = hd5[key]\n",
    "            open_hd5s.append(hd5)\n",
    "\n",
    "        # Add rows in csv with patient data accessible in patient dictionary\n",
    "        for csv_name, df, mrn_col in csv_data:\n",
    "            mask = df[mrn_col] == patient_id\n",
    "            if not mask.any():\n",
    "                continue\n",
    "            data[csv_name] = df[mask]\n",
    "\n",
    "        for i, tm in enumerate(tmaps):\n",
    "            _tensor = tm.tensor_from_file(tm, data)\n",
    "            if tm.time_series_limit is None:\n",
    "                _tensor = _tensor[None, ...]\n",
    "\n",
    "            for j in range(len(_tensor)):\n",
    "                try:\n",
    "                    _tensor[j] = tm.postprocess_tensor(\n",
    "                        tensor=_tensor[j],\n",
    "                        data=data,\n",
    "                        augment=augment,\n",
    "                        validate=validate,\n",
    "                        normalize=normalize,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    bad_idxs.append(j)\n",
    "            tensors[i] = _tensor\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    for hd5 in open_hd5s:\n",
    "        hd5.close()\n",
    "    for i in range(len(tensors)):\n",
    "        tensors[i] = np.delete(tensors[i], bad_idxs, axis=0)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_types = [tf.string if tm.is_language else tf.float32 for tm in tmaps]\n",
    "\n",
    "def wrapped(patient_id):\n",
    "    tensors = tf.py_function(\n",
    "        func=get_patient_tensors,\n",
    "        inp=[patient_id],\n",
    "        Tout=output_types,\n",
    "    )\n",
    "    in_tensors = {tm.input_name: tensors[i] for i, tm in enumerate(input_tmaps)}\n",
    "    out_tensors = {\n",
    "        tm.output_name: tensors[i+len(input_tmaps)] for i, tm in enumerate(output_tmaps)\n",
    "    }\n",
    "    return tf.data.Dataset.from_tensor_slices((in_tensors, out_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(list(patient_ids))\n",
    "dataset = dataset.flat_map(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# out = list(dataset.as_numpy_iterator())\n",
    "# print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4c3.datasets import make_dataset\n",
    "d, stats, cleanup = make_dataset(\n",
    "    data_split,\n",
    "    hd5_sources,\n",
    "    csv_sources,\n",
    "    patient_ids,\n",
    "    input_tmaps,\n",
    "    output_tmaps,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    cache=False\n",
    ")\n",
    "d = d.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r5\n",
    "out = list(d.as_numpy_iterator())\n",
    "print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dispatcher(port):\n",
    "    config = tf.data.experimental.service.DispatcherConfig(port=port)\n",
    "    d = tf.data.experimental.service.DispatchServer(config)\n",
    "    d.join()\n",
    "\n",
    "dispatcher = mp.Process(\n",
    "    target=run_dispatcher,\n",
    "    name='dispatcher',\n",
    "    args=(5050,),\n",
    ")\n",
    "dispatcher.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_worker(dispatcher_address):\n",
    "    config = tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address)\n",
    "    w = tf.data.experimental.service.WorkerServer(config)\n",
    "    w.join()\n",
    "\n",
    "workers = []\n",
    "for i in range(num_workers):\n",
    "    worker = mp.Process(\n",
    "        target=run_worker,\n",
    "        name=f'worker_{i}',\n",
    "        args=(\"localhost:5050\",)\n",
    "    )\n",
    "    worker.start()\n",
    "    workers.append(worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.service.distribute(\n",
    "    processing_mode=\"distributed_epoch\", service=\"grpc://localhost:5050\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r5\n",
    "out = list(dataset.as_numpy_iterator())\n",
    "print(len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
